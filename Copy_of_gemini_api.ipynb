{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mxsu/Week10-APIs/blob/main/Copy_of_gemini_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdQ4UKIGhRK6"
      },
      "source": [
        "# Using the Gemini API: Interactive Lab\n",
        "<a href=\"https://colab.research.google.com/github/IAT-ComputationalCreativity-Spring2025/Week10-APIs/blob/main/gemini_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "In this lab, we'll explore the basic usage of LLM APIs using Gemini's limited free tier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn4JRbE4hRK9"
      },
      "source": [
        "## Getting a key\n",
        "\n",
        "If you haven't already, go to https://aistudio.google.com/apikeyâ€‹ to sign in and create a new api key\n",
        "\n",
        "Make sure not to share this key publicly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiGSwrnOhRK_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# This will prompt for your API key but won't display what you type\n",
        "from getpass import getpass\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['GEMINI_API_KEY'] = getpass('Enter your Gemini API key: ')\n",
        "\n",
        "# Verify keys are set (without revealing them)\n",
        "print(f\"Gemini API key is set: {'Yes' if 'GEMINI_API_KEY' in os.environ else 'No'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8lVfSX1hRLB"
      },
      "source": [
        "## Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnnX2zQthRLC"
      },
      "outputs": [],
      "source": [
        "# Google Gemini API\n",
        "! pip install google-generativeai\n",
        "# OpenAI API (optional)\n",
        "#! pip install openai\n",
        "# Stability AI (optional)\n",
        "#! pip install stability-sdk\n",
        "\n",
        "# For interactive widgets in Jupyter\n",
        "! pip install ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwrKWk_whRLD"
      },
      "source": [
        "## Step 1: API Configuration Module\n",
        "\n",
        "Let's create a helper module to manage API access:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkS6gdmWhRLE"
      },
      "outputs": [],
      "source": [
        "# Try to get keys from multiple sources in order of security preference\n",
        "def get_api_key(service):\n",
        "    \"\"\"Get API key from various sources in order of security preference\"\"\"\n",
        "    if service.lower() == \"gemini\":\n",
        "        key = os.environ.get('GEMINI_API_KEY')\n",
        "\n",
        "        if key and key != \"your_gemini_api_key_here\":\n",
        "            return key\n",
        "\n",
        "        # No key found\n",
        "        raise ValueError(f\"No API key found for {service}. Please set up your API key using one of the methods in the notebook.\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported service: {service}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ3DTBIWhRLF"
      },
      "source": [
        "## Step 2: Testing API Connection\n",
        "\n",
        "Let's test our API connection using the Gemini API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwJD3v2uhRLG"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the current directory to path if needed\n",
        "sys.path.append(os.path.abspath(\"\"))\n",
        "\n",
        "# Test Gemini API\n",
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nzuf7uznhRLI"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # Configure the API\n",
        "    api_key = get_api_key(\"gemini\")\n",
        "    genai.configure(api_key=api_key)\n",
        "\n",
        "    # Test a simple query\n",
        "    model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "    response = model.generate_content(\"Write a haiku about artificial intelligence\")\n",
        "\n",
        "    print(\"API Connection Successful!\")\n",
        "    print(\"\\nHaiku response:\")\n",
        "    print(response.text)\n",
        "except Exception as e:\n",
        "    print(f\"Error connecting to API: {e}\")\n",
        "    print(\"\\nPlease check your API key configuration and try again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EncEbucPhRLK"
      },
      "source": [
        "## Step 3: API Rate Limit Monitoring and Error Handling\n",
        "\n",
        "Create utility functions for API rate limiting and error handling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oagnab2ZhRLL"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "from functools import wraps\n",
        "\n",
        "class APIRateLimiter:\n",
        "    def __init__(self, service_name, requests_per_minute=60):\n",
        "        self.service_name = service_name\n",
        "        self.min_interval = 60 / requests_per_minute  # seconds between requests\n",
        "        self.last_request_time = 0\n",
        "        self.request_count = 0\n",
        "\n",
        "    def request(self, func):\n",
        "        \"\"\"Decorator to manage API request rates\"\"\"\n",
        "        @wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            # Check if we need to wait\n",
        "            current_time = time.time()\n",
        "            elapsed = current_time - self.last_request_time\n",
        "\n",
        "            if elapsed < self.min_interval:\n",
        "                wait_time = self.min_interval - elapsed\n",
        "                print(f\"Rate limiting: waiting {wait_time:.2f} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "\n",
        "            # Make the request\n",
        "            self.request_count += 1\n",
        "            self.last_request_time = time.time()\n",
        "            print(f\"Making request #{self.request_count} to {self.service_name}\")\n",
        "\n",
        "            return func(*args, **kwargs)\n",
        "        return wrapper\n",
        "\n",
        "\n",
        "def retry_with_exponential_backoff(initial_delay=1, max_delay=60, max_retries=5):\n",
        "    \"\"\"Retry decorator with exponential backoff\"\"\"\n",
        "    def decorator(func):\n",
        "        @wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            delay = initial_delay\n",
        "            retries = 0\n",
        "\n",
        "            while retries < max_retries:\n",
        "                try:\n",
        "                    return func(*args, **kwargs)\n",
        "                except Exception as e:\n",
        "                    print(f\"Request failed: {e}\")\n",
        "                    retries += 1\n",
        "\n",
        "                    if retries >= max_retries:\n",
        "                        print(f\"Maximum retries ({max_retries}) exceeded.\")\n",
        "                        raise\n",
        "\n",
        "                    sleep_time = min(delay * (2 ** (retries - 1)) + random.uniform(0, 1), max_delay)\n",
        "                    print(f\"Retrying in {sleep_time:.2f} seconds... (Attempt {retries+1}/{max_retries})\")\n",
        "                    time.sleep(sleep_time)\n",
        "\n",
        "            return func(*args, **kwargs)  # One last try\n",
        "        return wrapper\n",
        "    return decorator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdcxIgyLhRLL"
      },
      "source": [
        "Let's test our rate limiter and retry logic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoKXqc4shRLM"
      },
      "outputs": [],
      "source": [
        "# Create rate limiters\n",
        "gemini_limiter = APIRateLimiter(\"Gemini\", requests_per_minute=60)\n",
        "\n",
        "@gemini_limiter.request\n",
        "def generate_with_gemini(prompt):\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text\n",
        "\n",
        "# Test with a few requests\n",
        "topics = [\"water\", \"air\", \"fire\"]\n",
        "for i in range(min(3, len(topics))):\n",
        "    try:\n",
        "        result = generate_with_gemini(f\"Give me one idea for a science project about {topics[i]}\")\n",
        "        print(f\"Result: {result[:100]}...\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbmkJThOhRLM"
      },
      "outputs": [],
      "source": [
        "# Test retry logic with an intentionally challenging request\n",
        "@retry_with_exponential_backoff(max_retries=3)\n",
        "def robust_generate_with_gemini(prompt):\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text\n",
        "\n",
        "try:\n",
        "    very_long_prompt = \"Explain quantum computing \" * 20\n",
        "    result = robust_generate_with_gemini(very_long_prompt)\n",
        "    print(f\"\\nSuccess! First 100 chars: {result[:100]}...\")\n",
        "except Exception as e:\n",
        "    print(f\"Final failure: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXbqGmeIhRLM"
      },
      "source": [
        "## Step 4: Creating a Comprehensive Helper Class\n",
        "\n",
        "Let's create a comprehensive AI client class for our lab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfFRiKpyhRLN"
      },
      "outputs": [],
      "source": [
        "class AIClient:\n",
        "    \"\"\"A unified client for interacting with generative AI APIs\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.services = {}\n",
        "        self.request_logs = []\n",
        "\n",
        "        # Initialize available services\n",
        "        self._init_gemini()\n",
        "\n",
        "    def _init_gemini(self):\n",
        "        \"\"\"Initialize Gemini API if key is available\"\"\"\n",
        "        try:\n",
        "            api_key = get_api_key(\"gemini\")\n",
        "            if api_key:\n",
        "                genai.configure(api_key=api_key)\n",
        "                self.services[\"gemini\"] = {\n",
        "                    \"text_model\": genai.GenerativeModel('gemini-2.0-flash'),\n",
        "                    \"rate_limit\": 60,  # requests per minute\n",
        "                    \"last_request\": 0,\n",
        "                }\n",
        "                print(\"Gemini API initialized successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not initialize Gemini API: {e}\")\n",
        "\n",
        "    def _rate_limit(self, service):\n",
        "        \"\"\"Apply rate limiting for the specified service\"\"\"\n",
        "        if service not in self.services:\n",
        "            raise ValueError(f\"Service {service} not initialized\")\n",
        "\n",
        "        min_interval = 60 / self.services[service][\"rate_limit\"]\n",
        "        current_time = time.time()\n",
        "        elapsed = current_time - self.services[service][\"last_request\"]\n",
        "\n",
        "        if elapsed < min_interval:\n",
        "            wait_time = min_interval - elapsed\n",
        "            print(f\"Rate limiting {service}: waiting {wait_time:.2f} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "\n",
        "        self.services[service][\"last_request\"] = time.time()\n",
        "\n",
        "    def generate_text(self, prompt, service=\"gemini\", max_retries=3):\n",
        "        \"\"\"Generate text from a text prompt using the specified service\"\"\"\n",
        "        if service not in self.services:\n",
        "            raise ValueError(f\"Service {service} not available\")\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                self._rate_limit(service)\n",
        "\n",
        "                if service == \"gemini\":\n",
        "                    response = self.services[service][\"text_model\"].generate_content(prompt)\n",
        "\n",
        "                    # Log the request\n",
        "                    self.request_logs.append({\n",
        "                        \"service\": service,\n",
        "                        \"prompt\": prompt[:100] + \"...\" if len(prompt) > 100 else prompt,\n",
        "                        \"timestamp\": time.time(),\n",
        "                        \"success\": True\n",
        "                    })\n",
        "\n",
        "                    return response.text\n",
        "                else:\n",
        "                    raise NotImplementedError(f\"Text generation not implemented for {service}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error ({attempt+1}/{max_retries}): {e}\")\n",
        "                if attempt == max_retries - 1:\n",
        "                    # Log the failed request\n",
        "                    self.request_logs.append({\n",
        "                        \"service\": service,\n",
        "                        \"prompt\": prompt[:100] + \"...\" if len(prompt) > 100 else prompt,\n",
        "                        \"timestamp\": time.time(),\n",
        "                        \"success\": False,\n",
        "                        \"error\": str(e)\n",
        "                    })\n",
        "                    raise\n",
        "                time.sleep(2 ** attempt)  # Exponential backoff\n",
        "\n",
        "    def get_usage_stats(self):\n",
        "        \"\"\"Return usage statistics for all services\"\"\"\n",
        "        stats = {}\n",
        "        for service in self.services:\n",
        "            service_logs = [log for log in self.request_logs if log[\"service\"] == service]\n",
        "            stats[service] = {\n",
        "                \"total_requests\": len(service_logs),\n",
        "                \"successful_requests\": len([log for log in service_logs if log[\"success\"]]),\n",
        "                \"failed_requests\": len([log for log in service_logs if not log[\"success\"]]),\n",
        "            }\n",
        "        return stats\n",
        "\n",
        "    def compare_responses(self, prompt, services=None):\n",
        "        \"\"\"Compare responses from multiple services\"\"\"\n",
        "        if services is None:\n",
        "            services = list(self.services.keys())\n",
        "\n",
        "        results = {}\n",
        "        for service in services:\n",
        "            try:\n",
        "                results[service] = self.generate_text(prompt, service=service)\n",
        "            except Exception as e:\n",
        "                results[service] = f\"Error: {e}\"\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1zyaQiihRLO"
      },
      "source": [
        "Let's test our AIClient:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THI-opqZhRLO"
      },
      "outputs": [],
      "source": [
        "# Create the client\n",
        "client = AIClient()\n",
        "\n",
        "# Use the client\n",
        "try:\n",
        "    response = client.generate_text(\"Explain the concept of API rate limiting in three sentences\")\n",
        "    print(response)\n",
        "\n",
        "    # Check usage\n",
        "    print(\"\\nUsage Statistics:\")\n",
        "    print(client.get_usage_stats())\n",
        "except Exception as e:\n",
        "    print(f\"Error using AIClient: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU3vCspAhRLO"
      },
      "source": [
        "## Step 5: An example use case -- ScientificArticleSummarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzYxuuoMhRLP"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "import ipywidgets as widgets\n",
        "import time\n",
        "import json\n",
        "\n",
        "class ScientificArticleSummarizer:\n",
        "    def __init__(self):\n",
        "        self.history = []  # Store previous summaries\n",
        "\n",
        "    def summarize_article(self, article_text, audience_level=\"general\", focus_areas=None, max_length=500):\n",
        "        \"\"\"\n",
        "        Summarizes scientific article for different audience levels with specified focus\n",
        "\n",
        "        Parameters:\n",
        "        - article_text (str): The text of the article to summarize\n",
        "        - audience_level (str): \"general\", \"undergraduate\", \"graduate\", or \"expert\"\n",
        "        - focus_areas (list): Specific aspects to focus on (e.g. [\"methodology\", \"results\"])\n",
        "        - max_length (int): Maximum length of summary in words\n",
        "\n",
        "        Returns:\n",
        "        - str: The generated summary\n",
        "        \"\"\"\n",
        "\n",
        "        # Set default focus areas if none provided\n",
        "        if focus_areas is None:\n",
        "            focus_areas = [\"main findings\", \"methodology\", \"significance\"]\n",
        "\n",
        "        # Determine language style based on audience level\n",
        "        language_style = {\n",
        "            \"general\": \"simple language avoiding technical jargon\",\n",
        "            \"undergraduate\": \"introductory academic language with basic field-specific terms\",\n",
        "            \"graduate\": \"advanced academic language with field-specific terminology\",\n",
        "            \"expert\": \"specialized technical language appropriate for experts in the field\"\n",
        "        }.get(audience_level, \"clear and concise language\")\n",
        "\n",
        "        # Build the prompt\n",
        "        prompt = f\"\"\"\n",
        "        Please summarize the following scientific article for a {audience_level} audience.\n",
        "        Use {language_style}.\n",
        "\n",
        "        Focus specifically on: {\", \".join(focus_areas)}.\n",
        "        Keep the summary under {max_length} words.\n",
        "\n",
        "        Structure your response with clear sections and bullet points where appropriate.\n",
        "\n",
        "        Article:\n",
        "        {article_text[:5000]}...\n",
        "        \"\"\"\n",
        "\n",
        "        if len(article_text) > 5000:\n",
        "            prompt += \"\\n[Note: Article text was truncated due to length. This summary is based on the first portion of the article.]\"\n",
        "\n",
        "        # Generate the summary\n",
        "        try:\n",
        "            summary = client.generate_text(prompt)\n",
        "\n",
        "            # Add to history\n",
        "            self.history.append({\n",
        "                \"timestamp\": time.time(),\n",
        "                \"audience_level\": audience_level,\n",
        "                \"focus_areas\": focus_areas,\n",
        "                \"summary_length\": len(summary.split()),\n",
        "                \"summary\": summary\n",
        "            })\n",
        "\n",
        "            return summary\n",
        "        except Exception as e:\n",
        "            return f\"Error generating summary: {str(e)}\"\n",
        "\n",
        "    def extract_key_elements(self, article_text):\n",
        "        \"\"\"\n",
        "        Extracts key elements from the article such as:\n",
        "        - Main findings\n",
        "        - Methodology\n",
        "        - Limitations\n",
        "        - Future research directions\n",
        "        - Practical implications\n",
        "\n",
        "        Returns a structured dictionary of these elements\n",
        "        \"\"\"\n",
        "\n",
        "        prompt = \"\"\"\n",
        "        Analyze the following scientific article and extract these key elements:\n",
        "        1. Main Findings: The primary results or discoveries reported\n",
        "        2. Methodology: The research approach, methods, and techniques used\n",
        "        3. Limitations: Any constraints, weaknesses, or limitations acknowledged\n",
        "        4. Future Research: Suggested directions for further study\n",
        "        5. Practical Implications: Real-world applications or consequences\n",
        "\n",
        "        Format your response as a structured JSON object with these elements as keys.\n",
        "        Keep each element concise (2-3 sentences).\n",
        "\n",
        "        Article:\n",
        "        \"\"\" + article_text[:5000]\n",
        "\n",
        "        try:\n",
        "            response = client.generate_text(prompt)\n",
        "\n",
        "            # Try to parse the response as JSON\n",
        "            # If it's not perfectly formatted JSON, we'll need to extract it\n",
        "            try:\n",
        "                # Look for a JSON block in the response\n",
        "                import re\n",
        "                json_match = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', response)\n",
        "                if json_match:\n",
        "                    json_str = json_match.group(1)\n",
        "                else:\n",
        "                    json_str = response\n",
        "\n",
        "                # Clean up and parse\n",
        "                return json.loads(json_str)\n",
        "            except json.JSONDecodeError:\n",
        "                # If parsing fails, return the raw response\n",
        "                return {\"error\": \"Could not parse structured data\", \"raw_response\": response}\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\"error\": f\"Error extracting key elements: {str(e)}\"}\n",
        "\n",
        "    def generate_follow_up_questions(self, article_text, num_questions=5):\n",
        "        \"\"\"\n",
        "        Generates follow-up questions that might be asked after reading the article\n",
        "        \"\"\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Based on the following scientific article, generate {num_questions} thoughtful follow-up questions that:\n",
        "        1. Probe deeper into the methodology or findings\n",
        "        2. Address potential limitations or gaps\n",
        "        3. Explore implications or applications of the research\n",
        "        4. Connect this research to broader scientific contexts\n",
        "        5. Suggest future research directions\n",
        "\n",
        "        Format each question as a separate numbered point.\n",
        "\n",
        "        Article:\n",
        "        {article_text[:5000]}\n",
        "        \"\"\"\n",
        "\n",
        "        return client.generate_text(prompt)\n",
        "\n",
        "    def compare_prompt_techniques(self, article_text, techniques=None):\n",
        "        \"\"\"\n",
        "        Compares different prompt engineering techniques on the same article\n",
        "        Returns a dictionary with results from each technique\n",
        "        \"\"\"\n",
        "        if techniques is None:\n",
        "            techniques = {\n",
        "                \"Basic\": \"Summarize this scientific article\",\n",
        "                \"Specific\": \"Summarize this scientific article, focusing on the methodology and findings\",\n",
        "                \"Role-based\": \"You are a scientific research assistant helping a professor. Summarize this article highlighting key contributions\",\n",
        "                \"Step-by-step\": \"First, identify the research question. Second, describe the methodology. Third, explain the results. Finally, summarize the conclusions.\",\n",
        "                \"Few-shot\": \"Example summary 1: [Paper title] investigated [topic] using [method] and found [result].\\nExample summary 2: The researchers explored [topic] through [method], revealing [result].\\nNow summarize this article in a similar style:\"\n",
        "            }\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        for name, prompt_template in techniques.items():\n",
        "            full_prompt = f\"{prompt_template}\\n\\nArticle:\\n{article_text[:3000]}\"\n",
        "\n",
        "            try:\n",
        "                results[name] = client.generate_text(full_prompt)\n",
        "            except Exception as e:\n",
        "                results[name] = f\"Error: {str(e)}\"\n",
        "\n",
        "        return results\n",
        "\n",
        "    def get_citation_recommendation(self, article_text):\n",
        "        \"\"\"Recommends how to cite this article based on its content\"\"\"\n",
        "\n",
        "        prompt = \"\"\"\n",
        "        Based on the content of this scientific article, generate:\n",
        "        1. An APA style citation (assume current year if publication date isn't mentioned)\n",
        "        2. Three key points that would be most relevant to cite from this paper\n",
        "        3. Potential fields or topics where citing this article would be appropriate\n",
        "\n",
        "        Article:\n",
        "        \"\"\" + article_text[:4000]\n",
        "\n",
        "        return client.generate_text(prompt)\n",
        "\n",
        "    def summarize_with_visuals(self, article_text):\n",
        "        \"\"\"\n",
        "        Provides guidance on what visual elements would complement a summary\n",
        "        of this article (diagrams, charts, etc.)\n",
        "        \"\"\"\n",
        "\n",
        "        prompt = \"\"\"\n",
        "        Based on this scientific article, recommend 3-5 visual elements (diagrams,\n",
        "        charts, figures) that would best complement a summary of this research.\n",
        "\n",
        "        For each recommended visual:\n",
        "        1. Describe what the visual should show\n",
        "        2. Explain why this visualization would be valuable\n",
        "        3. Suggest a title and caption\n",
        "\n",
        "        Article:\n",
        "        \"\"\" + article_text[:4000]\n",
        "\n",
        "        return client.generate_text(prompt)\n",
        "\n",
        "# Create a simple demo UI with widgets\n",
        "def create_summarizer_ui():\n",
        "    \"\"\"Creates an interactive UI for the scientific article summarizer\"\"\"\n",
        "    # Create the summarizer\n",
        "    summarizer = ScientificArticleSummarizer()\n",
        "\n",
        "    # Create widgets\n",
        "    article_input = widgets.Textarea(\n",
        "        value='',\n",
        "        placeholder='Paste scientific article text here',\n",
        "        description='Article:',\n",
        "        layout={'width': '100%', 'height': '200px'}\n",
        "    )\n",
        "\n",
        "    audience_dropdown = widgets.Dropdown(\n",
        "        options=['general', 'undergraduate', 'graduate', 'expert'],\n",
        "        value='undergraduate',\n",
        "        description='Audience:',\n",
        "        layout={'width': '50%'}\n",
        "    )\n",
        "\n",
        "    focus_checkboxes = widgets.SelectMultiple(\n",
        "        options=['main findings', 'methodology', 'significance', 'limitations', 'future research', 'practical implications'],\n",
        "        value=['main findings', 'methodology', 'significance'],\n",
        "        description='Focus on:',\n",
        "        layout={'width': '50%'}\n",
        "    )\n",
        "\n",
        "    max_length_slider = widgets.IntSlider(\n",
        "        value=500,\n",
        "        min=100,\n",
        "        max=1000,\n",
        "        step=50,\n",
        "        description='Max length:',\n",
        "        layout={'width': '50%'}\n",
        "    )\n",
        "\n",
        "    summarize_button = widgets.Button(\n",
        "        description='Summarize Article',\n",
        "        button_style='primary',\n",
        "        icon='file-text'\n",
        "    )\n",
        "\n",
        "    extract_button = widgets.Button(\n",
        "        description='Extract Key Elements',\n",
        "        button_style='info',\n",
        "        icon='list'\n",
        "    )\n",
        "\n",
        "    questions_button = widgets.Button(\n",
        "        description='Generate Questions',\n",
        "        button_style='success',\n",
        "        icon='question'\n",
        "    )\n",
        "\n",
        "    compare_button = widgets.Button(\n",
        "        description='Compare Prompt Techniques',\n",
        "        button_style='warning',\n",
        "        icon='random'\n",
        "    )\n",
        "\n",
        "    output_area = widgets.Output(\n",
        "        layout={'border': '1px solid #ddd', 'min_height': '200px', 'width': '100%', 'padding': '10px'}\n",
        "    )\n",
        "\n",
        "    # Define button actions\n",
        "    def on_summarize_clicked(b):\n",
        "        with output_area:\n",
        "            output_area.clear_output()\n",
        "            print(\"Generating summary...\")\n",
        "            summary = summarizer.summarize_article(\n",
        "                article_input.value,\n",
        "                audience_level=audience_dropdown.value,\n",
        "                focus_areas=list(focus_checkboxes.value),\n",
        "                max_length=max_length_slider.value\n",
        "            )\n",
        "            output_area.clear_output()\n",
        "            display(Markdown(summary))\n",
        "\n",
        "    def on_extract_clicked(b):\n",
        "        with output_area:\n",
        "            output_area.clear_output()\n",
        "            print(\"Extracting key elements...\")\n",
        "            elements = summarizer.extract_key_elements(article_input.value)\n",
        "            output_area.clear_output()\n",
        "\n",
        "            if isinstance(elements, dict) and \"error\" not in elements:\n",
        "                for key, value in elements.items():\n",
        "                    display(Markdown(f\"**{key}**: {value}\"))\n",
        "            else:\n",
        "                print(elements)\n",
        "\n",
        "    def on_questions_clicked(b):\n",
        "        with output_area:\n",
        "            output_area.clear_output()\n",
        "            print(\"Generating follow-up questions...\")\n",
        "            questions = summarizer.generate_follow_up_questions(article_input.value)\n",
        "            output_area.clear_output()\n",
        "\n",
        "            if isinstance(questions, list):\n",
        "                for i, question in enumerate(questions, 1):\n",
        "                    display(Markdown(f\"{i}. {question}\"))\n",
        "            else:\n",
        "                display(Markdown(questions))\n",
        "\n",
        "    def on_compare_clicked(b):\n",
        "        with output_area:\n",
        "            output_area.clear_output()\n",
        "            print(\"Comparing prompt techniques...\")\n",
        "            results = summarizer.compare_prompt_techniques(article_input.value)\n",
        "            output_area.clear_output()\n",
        "\n",
        "            for technique, result in results.items():\n",
        "                display(Markdown(f\"### Technique: {technique}\"))\n",
        "                display(Markdown(result))\n",
        "                display(Markdown(\"---\"))\n",
        "\n",
        "    # Connect buttons to actions\n",
        "    summarize_button.on_click(on_summarize_clicked)\n",
        "    extract_button.on_click(on_extract_clicked)\n",
        "    questions_button.on_click(on_questions_clicked)\n",
        "    compare_button.on_click(on_compare_clicked)\n",
        "\n",
        "    # Create tabs for different functionalities\n",
        "    tab_summarize = widgets.VBox([\n",
        "        widgets.HBox([audience_dropdown, max_length_slider]),\n",
        "        focus_checkboxes,\n",
        "        summarize_button\n",
        "    ])\n",
        "\n",
        "    tab_analyze = widgets.VBox([\n",
        "        extract_button,\n",
        "        questions_button,\n",
        "        compare_button\n",
        "    ])\n",
        "\n",
        "    tabs = widgets.Tab(children=[tab_summarize, tab_analyze])\n",
        "    tabs.set_title(0, 'Summarize')\n",
        "    tabs.set_title(1, 'Analyze')\n",
        "\n",
        "    # Assemble the complete UI\n",
        "    ui = widgets.VBox([\n",
        "        widgets.HTML(\"<h2>Scientific Article Summarizer</h2>\"),\n",
        "        article_input,\n",
        "        tabs,\n",
        "        widgets.HTML(\"<h3>Output:</h3>\"),\n",
        "        output_area\n",
        "    ])\n",
        "\n",
        "    return ui"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOS-JAOKhRLQ"
      },
      "source": [
        "## Step 6: Try it!\n",
        "\n",
        "Copy in the text from a scientific article (or just generate one below)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUX7XThYhRLR"
      },
      "outputs": [],
      "source": [
        "client.generate_text(\"Make up a short scientific paper\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXZ-xaAHhRLR"
      },
      "outputs": [],
      "source": [
        "# To run the interactive UI in a notebook:\n",
        "from IPython.display import display\n",
        "\n",
        "ui = create_summarizer_ui()\n",
        "display(ui)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "iat460",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}